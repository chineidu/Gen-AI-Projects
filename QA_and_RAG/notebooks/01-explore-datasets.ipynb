{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Data Exploration -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Literal, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### SQL Database(s) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/Gen-AI-Projects/QA_and_RAG\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=1)\n",
    "\n",
    "\n",
    "from src.db_utils import SQLFromTabularData\n",
    "from src.chatbot import Chatbot, get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating/updating the DB: Table 'titanic' already exists.\n",
      "DB Path: sqlite:///../data/flat_files/stored_data.db\n",
      "================================\n",
      "Available table Names: ['breast_cancer', 'diabetes', 'titanic']\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "create_db: SQLFromTabularData = SQLFromTabularData(\n",
    "    file_path=\"../data/flat_files/titanic-data.csv\",\n",
    "    db_path=\"../data/flat_files/stored_data.db\",\n",
    "    table_name=\"titanic\",\n",
    ")\n",
    "\n",
    "\n",
    "# create_db: SQLFromTabularData = SQLFromTabularData(\n",
    "#     file_path=\"../data/flat_files/titanic-data.csv\",\n",
    "#     db_path=\"../data/flat_files/stored_data.db\",\n",
    "#     table_name=\"titanic\",\n",
    "# )\n",
    "\n",
    "\n",
    "create_db.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/Gen-AI-Projects\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "from QA_and_RAG import PACKAGE_ROOT_PATH\n",
    "from QA_and_RAG.src.utils.utilities import ProcessFiles\n",
    "from config import config, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WORKING_DIR\"] = settings.WORKING_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/Gen-AI-Projects//QA_and_RAG\n"
     ]
    }
   ],
   "source": [
    "print(config.QA_and_RAG.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"A singleton class for managing ML model and its dependencies.\n",
    "\n",
    "    This class ensures only one instance of the model and its dependencies\n",
    "    are loaded in memory at any time.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    _instance : ModelManager | None\n",
    "        Singleton instance of the class\n",
    "    _model : Chatbot | None\n",
    "        The model\n",
    "    \"\"\"\n",
    "\n",
    "    _instance: \"ModelManager | None\" = None\n",
    "    _model: \"Chatbot | None\" = None\n",
    "\n",
    "    def __new__(cls) -> \"ModelManager\":\n",
    "        \"\"\"Create a new instance of ModelManager if one doesn't exist.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ModelManager\n",
    "            The singleton instance of ModelManager\n",
    "        \"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance._load_model()\n",
    "        return cls._instance\n",
    "\n",
    "    def _load_model(self) -> None:\n",
    "        \"\"\"Load the model and its dependencies from disk.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        HTTPException\n",
    "            If there is an error loading the model or dependencies\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Loading chatbot\")\n",
    "            self._model = get_response(chatbot, message, chat_type, app_functionality)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading chatbot: {e}\")\n",
    "            raise Exception(\"Error loading chatbot\")\n",
    "\n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"Clear the cached chatbot and reload them.\"\"\"\n",
    "        self._model = None\n",
    "        self._load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading chatbot: get_response() missing 4 required positional arguments: 'chatbot', 'message', 'chat_type', and 'app_functionality'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error loading chatbot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 46\u001b[0m, in \u001b[0;36mModelManager._load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading chatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: get_response() missing 4 required positional arguments: 'chatbot', 'message', 'chat_type', and 'app_functionality'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mModelManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m m\n",
      "Cell \u001b[0;32mIn[25], line 33\u001b[0m, in \u001b[0;36mModelManager.__new__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance\n",
      "Cell \u001b[0;32mIn[25], line 50\u001b[0m, in \u001b[0;36mModelManager._load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading chatbot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading chatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Error loading chatbot"
     ]
    }
   ],
   "source": [
    "m = ModelManager()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "db_path: str = \"../data/sql/chinook.db\"\n",
    "conn = create_engine(f\"sqlite:///{db_path}\")\n",
    "query: str = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "pl.read_database(query=query, connection=conn.connect()).to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Track LIMIT 10\"\n",
    "\n",
    "pl.read_database(query=query, connection=conn.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM Artist LIMIT 10\"\n",
    "\n",
    "pl.read_database(query=query, connection=conn.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp: str = \"../data/flat_files/breast-cancer.csv\"\n",
    "df: pl.DataFrame = pl.read_csv(fp)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires Pyarrow\n",
    "try:\n",
    "    full_db_path: str = \"sqlite:///test.db\"\n",
    "    df.head().write_database(table_name=\"sample_table\", connection=full_db_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect, Inspector\n",
    "from pydantic import BaseModel, computed_field, Field, PrivateAttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_engine(full_db_path)\n",
    "insp: Inspector = inspect(conn)\n",
    "\n",
    "insp.get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "\n",
    "config: DictConfig = OmegaConf.load(\"../../config/config.yaml\")\n",
    "\n",
    "print(OmegaConf.to_yaml(config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = str(ROOT_PATH / \"/data/sql/chinook.db\")\n",
    "file_path\n",
    "\n",
    "# ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=settings.OPENAI_API_KEY, temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "chat_type: Literal[\"qa-with-stored-sql-db\", \"qa-with-flat-file-sql-db\"] = (\n",
    "    \"qa-with-stored-sql-db\"\n",
    ")\n",
    "app_functionality: str = \"chat\"\n",
    "message: str = \"How many employees are there\"\n",
    "\n",
    "if app_functionality == \"chat\":\n",
    "    if chat_type == \"qa-with-stored-sql-db\":\n",
    "        # if os.path.exists(config.QA_and_RAG.chinook_db_path):\n",
    "        if os.path.exists(f\"../{file_path}\"):  # for jupyter notebook\n",
    "\n",
    "            # db = SQLDatabase.from_uri(config.QA_and_RAG.chinook_db_path)\n",
    "            db = SQLDatabase.from_uri(\n",
    "                f\"sqlite:///../{file_path}\"\n",
    "            )  # for jupyter notebook\n",
    "            exec_query = QuerySQLDatabaseTool(db=db)\n",
    "            write_query = create_sql_query_chain(llm, db=db)\n",
    "\n",
    "            answer_prompt = PromptTemplate.from_template(\n",
    "                template=config.QA_and_RAG.sql_agent_prompt\n",
    "            )\n",
    "\n",
    "            answer = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "            def execute_and_format(data: dict[str, str]) -> str:\n",
    "                \"\"\"Execute SQL query and format the answer.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                data : dict[str, str]\n",
    "                    Dictionary containing 'query' and 'question' keys with string values.\n",
    "                    The 'query' value may optionally start with 'SQLQuery:'.\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                str\n",
    "                    Formatted answer based on the question and query result.\n",
    "                \"\"\"\n",
    "                # Execute query\n",
    "                sql: str = data[\"query\"]\n",
    "                if sql.startswith(\"SQLQuery:\"):\n",
    "                    sql = sql.replace(\"SQLQuery:\", \"\").strip()\n",
    "                result: str = exec_query.invoke(sql)\n",
    "\n",
    "                return answer.invoke({\"question\": data[\"question\"], \"result\": result})\n",
    "\n",
    "            chain = (\n",
    "                RunnablePassthrough()\n",
    "                # Add the key `query` to the input dict\n",
    "                .assign(query=write_query)\n",
    "                # Chain the output of the prev step with the next step\n",
    "                .pipe(RunnableLambda(execute_and_format))\n",
    "            )\n",
    "\n",
    "            langchain.debug = True\n",
    "            response = chain.invoke({\"question\": message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _qa_sql_agent(db_path: str, config: DictConfig, message: str, llm: Any) -> str:\n",
    "    db = SQLDatabase.from_uri(f\"sqlite:///../{db_path}\")  # for jupyter notebook\n",
    "    print(db.dialect)\n",
    "    print(db.get_usable_table_names())\n",
    "    exec_query = QuerySQLDatabaseTool(db=db)\n",
    "    write_query = create_sql_query_chain(llm, db=db)\n",
    "\n",
    "    answer_prompt = PromptTemplate.from_template(\n",
    "        template=config.QA_and_RAG.sql_agent_prompt\n",
    "    )\n",
    "\n",
    "    answer = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "    def execute_and_format(data: dict[str, str]) -> str:\n",
    "        \"\"\"Execute SQL query and format the answer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dict[str, str]\n",
    "            Dictionary containing 'query' and 'question' keys with string values.\n",
    "            The 'query' value may optionally start with 'SQLQuery:'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Formatted answer based on the question and query result.\n",
    "        \"\"\"\n",
    "        # Execute query\n",
    "        sql: str = data[\"query\"]\n",
    "        print(\"===\" * 20)\n",
    "        print(\"sql: \", sql)\n",
    "        if (\n",
    "            sql.startswith(\"SQLQuery:\")\n",
    "            or sql.startswith(\"sql:\")\n",
    "            or sql.__contains__(\"```\")\n",
    "        ):\n",
    "            sql = (\n",
    "                sql.replace(\"SQLQuery:\", \"\")\n",
    "                .replace(\"sql:\", \"\")\n",
    "                .replace(\"```sql\", \"\")\n",
    "                .replace(\"```\", \"\")\n",
    "                .strip()\n",
    "            )\n",
    "        result: str = exec_query.invoke(sql)\n",
    "\n",
    "        return answer.invoke({\"question\": data[\"question\"], \"result\": result})\n",
    "\n",
    "    chain = (\n",
    "        RunnablePassthrough()\n",
    "        # Add the key `query` to the input dict\n",
    "        .assign(query=write_query)\n",
    "        # Chain the output of the prev step with the next step\n",
    "        .pipe(RunnableLambda(execute_and_format))\n",
    "    )\n",
    "    response: str = chain.invoke({\"question\": message})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp: str = \"../data/flat_files/titanic.csv\"\n",
    "df: pl.DataFrame = pl.read_csv(fp)\n",
    "\n",
    "# print(df[\"BloodPressure\"].mean())\n",
    "df.filter(pl.col(\"Sex\").eq(\"female\")).describe()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_path: str = \"/data/flat_files/stored_data.db\"\n",
    "# message: str = \"What is the total smoothnessmean?\"\n",
    "# _qa_sql_agent(db_path=db_path, config=config, message=message, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ChatType(Enum):\n",
    "    qa_with_stored_sql_db = \"q&a-with-stored-sql-db\"\n",
    "    qa_with_stored_flat_file_sql_db = \"q&a-with-stored-flat-file-sql-db\"\n",
    "    qa_with_uploaded_flat_file_sql_db = \"q&a-with-uploaded-flat-file-sql-db\"\n",
    "\n",
    "\n",
    "chat_type: Literal[\n",
    "    ChatType.qa_with_stored_sql_db,\n",
    "    ChatType.qa_with_stored_flat_file_sql_db,\n",
    "    ChatType.qa_with_uploaded_flat_file_sql_db,\n",
    "] = ChatType.qa_with_stored_sql_db.value\n",
    "app_functionality: str = \"chat\"\n",
    "message: str = \"How many employees are there\"\n",
    "chatbot: list[str] = []\n",
    "file_path: str = str(ROOT_PATH / \"/data/sql/chinook.db\")\n",
    "flat_file_path: str = str(ROOT_PATH / \"/data/sql/chinook.db\")\n",
    "\n",
    "if app_functionality == \"chat\":\n",
    "    if chat_type == ChatType.qa_with_stored_sql_db.value:\n",
    "        # if os.path.exists(config.QA_and_RAG.chinook_db_path):\n",
    "        if os.path.exists(f\"../{file_path}\"):  # for jupyter notebook\n",
    "            response = _qa_sql_agent(\n",
    "                db_path=file_path, config=config, message=message, llm=llm\n",
    "            )\n",
    "        else:\n",
    "            chatbot.append(\n",
    "                (\n",
    "                    message,\n",
    "                    f\"SQL DB does not exist. Please first create the `chinook` db\",\n",
    "                )\n",
    "            )\n",
    "    # elif chat_type == \"qa-with-flat-file-sql-db\":\n",
    "    elif chat_type == ChatType.qa_with_uploaded_flat_file_sql_db.value:\n",
    "        if os.path.exists(f\"../{file_path}\"):  # for jupyter notebook\n",
    "            db = SQLDatabase.from_uri(f\"sqlite:///../{file_path}\")\n",
    "            print(db.dialect)\n",
    "        else:\n",
    "            chatbot.append(\n",
    "                (\n",
    "                    message,\n",
    "                    f\"SQL DB from does not exist. Please first create the `chinook` db\",\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(db_path: str, llm: Any) -> tuple[Any, ...]:\n",
    "    db = SQLDatabase.from_uri(f\"sqlite:///../{db_path}\")  # for jupyter notebook\n",
    "    print(db.dialect)\n",
    "    return db, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path: str = \"/data/flat_files/stored_data.db\"\n",
    "message: str = \"How many people survived in the Titanic?\"\n",
    "message: str = \"What is the average blood pressure?\"\n",
    "\n",
    "_qa_sql_agent(db_path=db_path, config=config, message=message, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path: str = \"/data/flat_files/stored_data.db\"\n",
    "# message: str = \"How many people survived in the Titanic?\"\n",
    "db, llm = my_func(db_path=db_path, llm=llm)\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=llm, db=db, agent_type=\"openai-tools\", verbose=False\n",
    ")\n",
    "response = agent_executor.invoke(message)[\"output\"]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "\n",
    "query_prompt_template: ChatPromptTemplate = hub.pull(\n",
    "    \"langchain-ai/sql-query-system-prompt\"\n",
    ")\n",
    "# console.print(query_prompt_template.messages[0])\n",
    "query_prompt_template.messages[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "\n",
    "\n",
    "chat_llm: ChatOpenAI = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", api_key=settings.OPENAI_API_KEY, temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: str\n",
    "    result: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class QueryOutput(TypedDict):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
    "\n",
    "\n",
    "def write_query(state: State):\n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "    prompt = query_prompt_template.invoke(\n",
    "        {\n",
    "            \"dialect\": db.dialect,\n",
    "            \"top_k\": 10,\n",
    "            \"table_info\": db.get_table_info(),\n",
    "            \"input\": state[\"question\"],\n",
    "        }\n",
    "    )\n",
    "    structured_llm = chat_llm.with_structured_output(QueryOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    return {\"query\": result[\"query\"]}\n",
    "\n",
    "\n",
    "def execute_query(state: State):\n",
    "    \"\"\"Execute SQL query.\"\"\"\n",
    "    execute_query_tool = QuerySQLDatabaseTool(db=db)\n",
    "    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\n",
    "\n",
    "\n",
    "def generate_answer(state: State):\n",
    "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following user question, corresponding SQL query, \"\n",
    "        \"and SQL result, answer the user question.\\n\\n\"\n",
    "        f'Question: {state[\"question\"]}\\n'\n",
    "        f'SQL Query: {state[\"query\"]}\\n'\n",
    "        f'SQL Result: {state[\"result\"]}'\n",
    "    )\n",
    "    response = chat_llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data: State = State(\n",
    "    {\"question\": \"How many men were there in the Titanic that survived?\"}\n",
    ")\n",
    "query: str = write_query(state_data)[\"query\"]\n",
    "state_data[\"query\"] = query\n",
    "\n",
    "query_result: dict[str, Any] = execute_query(state_data)[\"result\"]\n",
    "state_data[\"result\"] = query_result\n",
    "\n",
    "generate_answer(state=state_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Remove the Titanic dataset.\n",
    "# 2. Update the sql agent using the latest version.\n",
    "# 3. Create the Python scripts/modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
