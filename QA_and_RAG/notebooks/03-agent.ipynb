{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Literal, Optional, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/Gen-AI-Projects\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/neidu/Desktop/Projects/Personal/My_Projects/Gen-AI-Projects/QA_and_RAG')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "from QA_and_RAG import ROOT_PATH\n",
    "from config import settings\n",
    "\n",
    "\n",
    "ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    "    RunnableSequence,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "\n",
    "config: DictConfig = OmegaConf.load(\"../../config/config.yaml\")\n",
    "\n",
    "print(OmegaConf.to_yaml(config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = str(ROOT_PATH / \"/data/sql/chinook.db\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=settings.OPENAI_API_KEY, temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Approach (Previous Versions of LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1: str = \"\"\"Based on the database query result, provide a complete sentence\n",
    "    answer to the user''s question.\n",
    "\n",
    "    Question: {question}\n",
    "    SQL Result: {result}\n",
    "    Answer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _qa_sql_agent(db_path: str, prompt: str, message: str, llm: Any) -> str:\n",
    "    \"\"\"Execute SQL queries and format answers using a language model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_path : str\n",
    "        Path to the SQLite database file.\n",
    "    prompt : str\n",
    "        Template string for formatting the answer.\n",
    "    message : str\n",
    "        User question to be answered.\n",
    "    llm : Any\n",
    "        Language model instance for generating SQL queries and answers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Formatted response to the user's question.\n",
    "    \"\"\"\n",
    "    db: SQLDatabase = SQLDatabase.from_uri(\n",
    "        f\"sqlite:///../{db_path}\"\n",
    "    )  # for jupyter notebook\n",
    "    print(f\"DB Dialect: {db.dialect}\\nTable Names: {db.get_usable_table_names()}\")\n",
    "\n",
    "    exec_query: QuerySQLDatabaseTool = QuerySQLDatabaseTool(db=db)\n",
    "    write_query: RunnableSequence = create_sql_query_chain(llm, db=db)\n",
    "\n",
    "    answer_prompt: PromptTemplate = PromptTemplate.from_template(template=prompt)\n",
    "\n",
    "    answer: RunnableSequence = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "    def execute_and_format(data: dict[str, str]) -> str:\n",
    "        \"\"\"Execute SQL query and format the answer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dict[str, str]\n",
    "            Dictionary containing 'query' and 'question' keys with string values.\n",
    "            The 'query' value may optionally start with 'SQLQuery:'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Formatted answer based on the question and query result.\n",
    "        \"\"\"\n",
    "        # Execute query\n",
    "        sql: str = data[\"query\"]\n",
    "        print(\"===\" * 20)\n",
    "        print(\"sql: \", sql)\n",
    "        if (\n",
    "            sql.startswith(\"SQLQuery:\")\n",
    "            or sql.startswith(\"sql:\")\n",
    "            or sql.__contains__(\"\")\n",
    "        ):\n",
    "            sql = (\n",
    "                sql.replace(\"SQLQuery:\", \"\")\n",
    "                .replace(\"sql:\", \"\")\n",
    "                .replace(\"sql\", \"\")\n",
    "                .replace(\"```\", \"\")\n",
    "                .strip()\n",
    "            )\n",
    "        result: str = exec_query.invoke(sql)\n",
    "\n",
    "        return answer.invoke({\"question\": data[\"question\"], \"result\": result})\n",
    "\n",
    "    chain: RunnableSequence = (\n",
    "        RunnablePassthrough()\n",
    "        # Add the key `query` to the input dict\n",
    "        .assign(query=write_query)\n",
    "        # Chain the output of the prev step with the next step\n",
    "        .pipe(RunnableLambda(execute_and_format))\n",
    "    )\n",
    "    response: str = chain.invoke({\"question\": message})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp: str = \"../data/flat_files/titanic-data.csv\"\n",
    "df: pl.DataFrame = pl.read_csv(fp)\n",
    "\n",
    "# print(df[\"BloodPressure\"].mean())\n",
    "df.filter(pl.col(\"sex\").eq(\"male\")).describe()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from QA_and_RAG.src.db_utils import SQLFromTabularData\n",
    "\n",
    "\n",
    "class ChatType(Enum):\n",
    "    qa_with_stored_sql_db = \"q&a-with-stored-sql-db\"\n",
    "    qa_with_stored_flat_file_sql_db = \"q&a-with-stored-flat-file-sql-db\"\n",
    "    qa_with_uploaded_flat_file_sql_db = \"q&a-with-uploaded-flat-file-sql-db\"\n",
    "\n",
    "\n",
    "chat_type: Literal[\n",
    "    ChatType.qa_with_stored_sql_db,\n",
    "    ChatType.qa_with_stored_flat_file_sql_db,\n",
    "    ChatType.qa_with_uploaded_flat_file_sql_db,\n",
    "] = ChatType.qa_with_stored_sql_db.value\n",
    "app_functionality: str = \"chat\"\n",
    "message: str = \"How many employees are there\"\n",
    "chatbot: list[str] = []\n",
    "file_path: str = str(ROOT_PATH / \"/data/sql/chinook.db\")\n",
    "flat_file_path: str = str(ROOT_PATH / \"/data/flat_files/stored_data.db\")\n",
    "\n",
    "if app_functionality == \"chat\":\n",
    "    if chat_type == ChatType.qa_with_stored_sql_db.value:\n",
    "        # if os.path.exists(config.QA_and_RAG.chinook_db_path):\n",
    "        if os.path.exists(f\"../{file_path}\"):  # for jupyter notebook\n",
    "            response = _qa_sql_agent(\n",
    "                db_path=file_path, prompt=prompt_1, message=message, llm=llm\n",
    "            )\n",
    "        else:\n",
    "            chatbot.append(\n",
    "                (\n",
    "                    message,\n",
    "                    f\"SQL DB does not exist. Please first create the `chinook` db\",\n",
    "                )\n",
    "            )\n",
    "    # elif chat_type == \"qa-with-flat-file-sql-db\":\n",
    "    elif chat_type == ChatType.qa_with_uploaded_flat_file_sql_db.value:\n",
    "        _db_path: str = \"../data/for_upload/uploaded_data.db\"\n",
    "        create_db: SQLFromTabularData = SQLFromTabularData(\n",
    "            file_path=\"uploaded_data.csv\",\n",
    "            db_path=_db_path,\n",
    "            table_name=\"your_table_name\",\n",
    "        )\n",
    "        if os.path.exists(f\"../{file_path}\"):  # for jupyter notebook\n",
    "            response = _qa_sql_agent(\n",
    "                db_path=_db_path, prompt=prompt_1, message=message, llm=llm\n",
    "            )\n",
    "        else:\n",
    "            chatbot.append(\n",
    "                (\n",
    "                    message,\n",
    "                    f\"SQL DB from does not exist. Please upload a valid `CSV`/`PARQUET` file.\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    elif chat_type == ChatType.qa_with_stored_flat_file_sql_db.value:\n",
    "        if os.path.exists(f\"../{file_path}\"):  # for jupyter notebook\n",
    "            response = _qa_sql_agent(\n",
    "                db_path=file_path, prompt=prompt_1, message=message, llm=llm\n",
    "            )\n",
    "        else:\n",
    "            chatbot.append(\n",
    "                (\n",
    "                    message,\n",
    "                    f\"SQL DB from does not exist. Please first create the `chinook` db\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    chatbot.append((message, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database(db_path: str) -> SQLDatabase:\n",
    "    db: SQLDatabase = SQLDatabase.from_uri(\n",
    "        f\"sqlite:///../{db_path}\"\n",
    "    )  # for jupyter notebook\n",
    "    print(db.dialect)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path: str = \"/data/flat_files/stored_data.db\"\n",
    "message: str = \"What is the average blood pressure?\"\n",
    "message: str = \"How many men survived in the Titanic?\"\n",
    "\n",
    "_qa_sql_agent(db_path=db_path, prompt=prompt_1, message=message, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Using Agent Executor (Old Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path: str = \"/data/flat_files/stored_data.db\"\n",
    "db: SQLDatabase = get_database(db_path=db_path)\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=llm, db=db, agent_type=\"openai-tools\", verbose=False\n",
    ")\n",
    "response = agent_executor.invoke(message)[\"output\"]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "\n",
    "query_prompt_template: ChatPromptTemplate = hub.pull(\n",
    "    \"langchain-ai/sql-query-system-prompt\"\n",
    ")\n",
    "# console.print(query_prompt_template.messages[0])\n",
    "query_prompt_template.messages[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "\n",
    "\n",
    "chat_llm: ChatOpenAI = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", api_key=settings.OPENAI_API_KEY, temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: str\n",
    "    result: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class QueryOutput(TypedDict):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
    "\n",
    "\n",
    "def write_query(state: State):\n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "    prompt = query_prompt_template.invoke(\n",
    "        {\n",
    "            \"dialect\": db.dialect,\n",
    "            \"top_k\": 10,\n",
    "            \"table_info\": db.get_table_info(),\n",
    "            \"input\": state[\"question\"],\n",
    "        }\n",
    "    )\n",
    "    structured_llm = chat_llm.with_structured_output(QueryOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    return {\"query\": result[\"query\"]}\n",
    "\n",
    "\n",
    "def execute_query(state: State):\n",
    "    \"\"\"Execute SQL query.\"\"\"\n",
    "    execute_query_tool = QuerySQLDatabaseTool(db=db)\n",
    "    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\n",
    "\n",
    "\n",
    "def generate_answer(state: State):\n",
    "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following user question, corresponding SQL query, \"\n",
    "        \"and SQL result, answer the user question.\\n\\n\"\n",
    "        f'Question: {state[\"question\"]}\\n'\n",
    "        f'SQL Query: {state[\"query\"]}\\n'\n",
    "        f'SQL Result: {state[\"result\"]}'\n",
    "    )\n",
    "    response = chat_llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data: State = State(\n",
    "    # {\"question\": \"How many men were there in the Titanic that survived?\"}\n",
    "    {\"question\": \"What is the ratio of men to women that survived the Titanic?\"}\n",
    ")\n",
    "query: str = write_query(state_data)[\"query\"]\n",
    "state_data[\"query\"] = query\n",
    "\n",
    "query_result: dict[str, Any] = execute_query(state_data)[\"result\"]\n",
    "state_data[\"result\"] = query_result\n",
    "\n",
    "generate_answer(state=state_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence(\n",
    "    [write_query, execute_query, generate_answer]\n",
    ")\n",
    "graph_builder.add_edge(START, \"write_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = False\n",
    "\n",
    "for step in graph.stream({\"question\": \"How many men did NOT survive the Titanic?\"}):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"How many employees are there?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UploadFile:\n",
    "    \"\"\"\n",
    "    A class that acts as a controller to run various file processing pipelines\n",
    "    based on the chatbot's current functionality when handling uploaded files.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def run_pipeline(files_dir: List, chatbot: List, chatbot_functionality: str):\n",
    "        \"\"\"\n",
    "        Run the appropriate pipeline based on chatbot functionality.\n",
    "\n",
    "        Args:\n",
    "            files_dir (List): List of paths to uploaded files.\n",
    "            chatbot (List): The current state of the chatbot's dialogue.\n",
    "            chatbot_functionality (str): A string specifying the chatbot's current functionality.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple of an empty string and the updated chatbot list, or None if functionality not matched.\n",
    "        \"\"\"\n",
    "        if chatbot_functionality == \"Process files\":\n",
    "            pipeline_instance = ProcessFiles(files_dir=files_dir, chatbot=chatbot)\n",
    "            input_txt, chatbot = pipeline_instance.run()\n",
    "            return input_txt, chatbot\n",
    "        else:\n",
    "            pass  # Other functionalities can be implemented here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
